import os
from groq import Groq
from openai import OpenAI
from google import genai
from google.genai import types
import json

# deployé–‹
# from api.vector_search import (
#     vector_search_light,
# )

#localé–‹
from vector_search import (
    vector_search_light,
)

# ç’°å¢ƒè®Šæ•¸è¨­å®š
GROQ_MODEL = "llama-3.3-70b-versatile"
GROQ_MODEL_AGENT = "llama-3.3-70b-versatile"
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
groq_client = Groq(api_key=GROQ_API_KEY)


def format_history_for_prompt(history: list) -> str:
    """åªæå– question èˆ‡ response æ¬„ä½ï¼Œæ ¼å¼åŒ–æˆå°è©±ç´€éŒ„å­—ä¸²"""
    history_lines = []
    for i, item in enumerate(history):
        q = item.get("question", "").strip()
        r = item.get("response", "").strip()
        if q and r:
            history_lines.append(f"{i+1}. å­¸ç”Ÿï¼š{q}\n   å­¸é•·å§ï¼š{r}")
    return "\n".join(history_lines) if history_lines else "ï¼ˆç„¡ï¼‰"


def clarify_question(user_input: str, user_history: list) -> str:
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

    model = "gemini-2.0-flash-lite"
    
    # ç”¨ f-string æ³¨å…¥ user_input
    prompt = f"""ä½ æ˜¯ä¸€ä½å­¸é•·å§ï¼Œæ“…é•·å”åŠ©å­¸å¼Ÿå¦¹é‡æ¸…å•é¡Œã€‚

        è«‹åˆ¤æ–·ä»¥ä¸‹å­¸ç”Ÿçš„æå•æ˜¯å¦æ¸…æ¥šï¼Œä¸¦ä¾ç…§ä»¥ä¸‹è¦å‰‡å›è¦†ï¼š
        1. å¦‚æœå•é¡Œæ¸…æ¥šï¼Œè«‹åªå›è¦†ã€Œå•é¡Œæ¸…æ¥šã€å››å€‹å­—ï¼Œ**ä¸è¦å¤šèªª**ã€‚
        2. å¦‚æœå•é¡Œä¸æ¸…æ¥šï¼ˆä¸å¤ å…·é«”ã€èªæ„ä¸æ˜ã€ç¼ºä¹ä¸Šä¸‹æ–‡ï¼‰ï¼Œè«‹çµ¦å‡ºç°¡å–®æ˜ç¢ºçš„å¼•å°èªï¼Œå¼•å°å­¸ç”Ÿè£œå……ä»–æƒ³å•çš„å…§å®¹ã€‚èªæ°£è¦åƒå­¸é•·å§ï¼Œå£èªåŒ–ä¸€é»ã€‚
        3. å¦‚æœçµåˆå°è©±ç´€éŒ„çš„å…§å®¹ï¼Œèƒ½å¤ é‡æ¸…å•é¡Œï¼Œè«‹ç›´æ¥çµ¦å‡ºç°¡å–®æ˜ç¢ºçš„å¼•å°èªã€‚

        å­¸ç”Ÿçš„å•é¡Œå¦‚ä¸‹ï¼š
        ã€Œ{user_input}ã€

        ä½ å€‘çš„å°è©±ç´€éŒ„ï¼ˆå¯åƒè€ƒæ˜¯å¦å·²æœ‰ä¸Šä¸‹æ–‡ï¼‰ï¼š
        {user_history}
        """

    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part(text=prompt)  # âœ… æ›´å®‰å…¨çš„ç”¨æ³•
            ]
        )
    ]

    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    full_response = "" # å„²å­˜å®Œæ•´çš„å›æ‡‰
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")  # è‹¥ä½ è¦å³æ™‚é¡¯ç¤ºç”¨é€™å€‹
        full_response += chunk.text

    return full_response.strip()


def get_groq_response(user_input: str, user_history: list) -> str:
    try:
        user_history = format_history_for_prompt(user_history)
        print("âš¡ Prompt ä¸Šä¸‹æ–‡ï¼š")
        print(user_history)
        
        clarification = clarify_question(user_input,user_history)
        if clarification != "å•é¡Œæ¸…æ¥šã€‚":
            return clarification, None
        
        if  clarification == "å•é¡Œæ¸…æ¥šã€‚":
            # å¦‚æœå•é¡Œæ¸…æ¥šï¼Œå‰‡ç›´æ¥ä½¿ç”¨åŸå§‹å•é¡Œé€²è¡Œå‘é‡æŸ¥è©¢
            search_result = vector_search_light(user_input)
        else:
            # å¦‚æœå•é¡Œä¸æ¸…æ¥šï¼Œå‰‡ä½¿ç”¨é‡æ¸…å¾Œçš„å•é¡Œé€²è¡Œå‘é‡æŸ¥è©¢
            search_result = vector_search_light(clarification)
            
        text = search_result.get("text", "æœªçŸ¥å…§å®¹")
        print(f"å‘é‡æŸ¥è©¢çµæœå…§å®¹: {text}")

        # å»ºç«‹ messages çµæ§‹
        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä½ä¸­æ–‡ç³»çµ±åˆ†æèˆ‡è¨­è¨ˆèª²ç¨‹çš„å¤§å­¸å­¸é•·å§ï¼Œæ­£åœ¨å¹«åŠ©å­¸å¼Ÿå¦¹è§£ç­”èˆ‡èª²å ‚å­¸ç¿’ã€å°ˆé¡Œå¯¦ä½œã€é–‹ç™¼ç¶“é©—ç›¸é—œçš„å•é¡Œã€‚\n"
                    "ä½ çš„èªæ°£è¦åƒå­¸é•·å§åˆ†äº«ç¶“é©—ä¸€æ¨£ï¼šå£èªåŒ–ã€æœ‰å¯¦éš›ä¾‹å­ã€ç°¡å–®æ˜ç¢ºã€é¼“å‹µå¾Œè¼©ã€æ§åˆ¶åœ¨ 3ï½5 å¥å…§ï¼Œä¸è¦èªªå¤ªå¤šå»¢è©±ã€‚"
                )
            },
            {
                "role": "user",
                "content": (
                    f"å­¸ç”Ÿçš„å•é¡Œæ˜¯ï¼š{user_input}\n\n"
                    f"è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å…§å®¹ä¾†å›ç­”ï¼Œä¸è¦æ†‘ç©ºçŒœæ¸¬ï¼Œåªèƒ½å¾è³‡æ–™ä¸­æ‰¾å‡ºç›¸é—œç¶“é©—ä¾†å›ç­”ï¼š\n{text}"
                )
            },
            {
                "role": "assistant",
                "content": (
                    f"ä½ å€‘çš„å°è©±ç´€éŒ„ï¼š{user_history}\n\n"
                )
            }
        ]

        # å°‡å‰ç«¯å‚³ä¾†çš„å°è©±æ­·å²æ’å…¥ messagesï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if isinstance(user_history, list):
            messages.extend(user_history)

        response = groq_client.chat.completions.create(
            model=GROQ_MODEL,
            messages=messages,
            temperature=0.7,
            top_p=0.9,
            max_tokens=1000
        )

        answer = response.choices[0].message.content
        print(f"Groq API å›æ‡‰: {answer}")

        return answer, search_result

    except Exception as e:
        raise RuntimeError(f"Error from Groq API: {str(e)}")

# token = os.environ["GITHUB_TOKEN"]
endpoint = "https://models.inference.ai.azure.com"
model_name = "gpt-4o-mini"

def format_context(matches: list[dict]) -> str:
    blocks = []
    for match in matches:
        source = match['metadata'].get('source', 'æœªçŸ¥æª”æ¡ˆ')
        index = int(match['metadata'].get('chunk_index', -1))
        interviewee = ", ".join(set(match['metadata'].get('interviewee', ['æœªçŸ¥'])))
        text = match['metadata'].get('text', '')

        block = f"ã€ä¾†è‡ª {source} ç¬¬{index}æ®µï¼ˆ{interviewee}ï¼‰ã€‘\n{text}"
        blocks.append(block)

    return "\n\n".join(blocks)

def format_history_for_chat(history):
    messages = []
    for pair in history:
        messages.append({"role": "user", "content": pair["user"]})
        messages.append({"role": "assistant", "content": pair["ai"]})
    return messages


V_SENPAI_SYSTEM_PROMPT = """
ä½ æ˜¯å­¸é•·å§Šæ¨¡æ“¬æ©Ÿå™¨äºº V-Senpaiï¼Œä¸€ä½èªªä¸­æ–‡çš„èª²å ‚åŠ©ç†ï¼Œæ ¹æ“šè¨ªè«‡è³‡æ–™ï¼Œå”åŠ©å­¸ç”Ÿäº†è§£ã€Œç³»çµ±åˆ†æèˆ‡è¨­è¨ˆã€èª²ç¨‹èˆ‡å°ˆé¡Œå¯¦ä½œä¹‹é–“çš„å·®ç•°èˆ‡ç¶“é©—ã€‚
åƒæ˜¯åœ¨èˆ‡å­¸å¼Ÿå¦¹èŠå¤©ä¸€æ¨£ï¼Œå›ç­”é•·åº¦è«‹æ§åˆ¶åœ¨ 3ï½5 å¥ä¹‹é–“ã€‚
å›æ‡‰æ™‚è«‹ä½¿ç”¨ã€æŸæŸå­¸é•·å§è¡¨ç¤ºâ‹¯â‹¯ã€ã€ã€æ ¹æ“šèª°èª°èª°çš„è¨ªè«‡ç´€éŒ„â‹¯â‹¯ã€é€™é¡èªªæ³•ï¼Œè«‹å‹¿å‡è£è‡ªå·±å°±æ˜¯å—è¨ªè€…æœ¬äººã€‚
ä½ çš„å›ç­”åªèƒ½æ ¹æ“š context_text ä¸­çš„è³‡æ–™å…§å®¹ï¼Œä¸å¯ä»¥è‡ªè¡Œæƒ³åƒæˆ–è£œå……è³‡æ–™ï¼Œè‹¥è³‡æ–™ä¸è¶³ï¼Œä¹Ÿè«‹å¦ç‡èªªæ˜ã€‚
"""

def get_openai_response(token: str, user_input: str, history: object) -> str:
    client = OpenAI(
        base_url=endpoint,
        api_key=token,
    )

    search_result = vector_search_light(user_input)
    print("ğŸ” æŸ¥è©¢çµæœå¦‚ä¸‹ï¼š\n")
    print(search_result["text"])
    
    # context_text = format_context(search_result.get("matches", [])) 
    context_text = search_result.get("text", "æŸ¥ç„¡è³‡æ–™ã€‚")
    
    # print("å‘é‡æŸ¥è©¢çµæœå…§å®¹", context_text)
    
    messages = [
    {"role": "system", "content": V_SENPAI_SYSTEM_PROMPT},
    # *format_history_for_chat(history),  # ğŸ‘ˆ å°‡æ­·å²å°è©±æ’å…¥
    {"role": "user", "content": user_input},
    {"role": "assistant", "content": context_text}
    ]

    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        temperature=1.0,
        top_p=1.0
    )

    print("æ©Ÿå™¨äººå›æ‡‰", response.choices[0].message.content)
    return response.choices[0].message.content , context_text



if __name__ == "__main__":
    user_input = "é—œæ–¼æ—èŠ¯ç·¹æœ‰æ²’æœ‰ä»€éº¼è³‡æ–™å‘¢ï¼Ÿ"
    response = get_openai_response("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", user_input, [])
    print(f"å›æ‡‰: {response}")
